[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{pro}'s high availability feature provides clustering for two separate instances of the repository manager. This 
feature enables a quicker backend among hosts, minimizing downtime and reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability in your repository manager. 
Usage of high availability clusters set the standard for automating the process, relieving teams of manual 
recovery techniques, and creating a binary storage for the entire suite of repository manager features and 
components.

IMPORTANT: {pro} customers can take advantage of the assistance of the support team.

[[why-high-availability]]
=== Why Cluster?

The biggest draw to a high performing repository manager is of a self-maintaining system with improved disaster 
recovery features. Additionally backup support, high availability clusters can be used for load balancing and 
failover initiative. Currently, you can only implement two nodes. But down the line, can expand the amount of 
physical or virtual servers you want to introduce to your clustering plan.

////
... or some better way to hint at component fabric
////

[[high-availability-purpose]]
=== What is the Purpose of High Availability?

The high availability feature provides a storage backend for two {nxrm} instances co-located in the same data 
center.

High availability implementations are made of servers, joined together but a common directory. The second cluster 
is implemented to achieve synchronization between both enabled clusters. The clusters (or servers, or nodes) 
communicate with each other to provide uptime and the assurance that if one goes down a duplicated node can be 
used.

Instead of manually managing your infrastructure, processes, and tools associated with numerous repository 
manager installations, {pro} high availability feature provides a replicated storage system that eliminates 
complexity and offers a simple and cost effective way to provision, configure, manage, and maintain multiple 
repositories.

This feature is enabled across your nodes via Hazelcast. Hazelcast provides an active/active approach, with an 
interface for distributed data structures and other aspects of in-memory computing.

When enabling active/active high availability in your two repository manager instances, they act as 
individual nodes, fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This awareness also translates to increased efficiency, scalability, and control, allowing nodes to automatically 
choose the fastest route to intercept component data between the two nodes. With additional customization, these 
purpose-driven nodes can configure prefetching of components even before a client tool requests it, leading to 
greater level of performance.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

{pro} is flexible, and can lend itself to a variety of different system architectures. So, before enabling this 
specific feature, determine which option is best for your existing architecture and system configuration before 
installing. There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them on two:

- separate physical servers
- virtual machines
- Docker instances

Your should recognize any challenges you may encounter when planning high availability implementation. You also 
need to be mindful of cluster performance, so your repository manager can remain at its most optimal speed.

Ultimately, the rule of thumb is you ensure the two nodes utilized share a file system for blob storage and that 
they exist on the same network. Subsequently, the key goals for enabling high availability are as follows:

- <<high-availability-install,Installing a secondary version>> of {pro}.
- <<high-availability-configuration,Configuring the second binary>> as its own independent server.
- <<high-availability-enable,Activating the clusters>> from the file `org.sonatype.nexus.cfg`.
- <<high-availability-startup,Starting>> and running the service upon activation.

[[high-availability-install]]
==== Installing a New Binary

Assuming you have an existing repository manager in place, download a brand new version of {pro}. 
Install this new binary in the same directory as your existing repository manager. You can refer to 
<<installation-archive>> or <<installation-installer>> for details on how to download and install.

As you install your new repository manager, be sure to mount it in the same directory as the existing repository 
manager. To properly configure a high-availability cluster the hosts in the cluster must all have access to the 
same shared storage. You need to share the nodes the directory location of your 
<<admin-repository-blobstores,blob store>>. You shouldn't configure your nodes to share an entire data directory. 
Each node should have its own independent non-conflicting OrientDB database.

For example, if you have two nodes -- a default called "node-a" and a new node called "node-b" -- and "node-a's"  
blob store directory located at `/foo/bar/blobs/foobar`. "Node-b" should be mounted to the same file system 
location.

[[high-availability-configuration]]
==== Configuring the Port of the Second Binary

After you extract your newly downloaded `tar.gz` of {pro}, you need to establish communication between your two 
nodes. To do this, you must distinguish the `application-port` of your existing repository manager's 
`org.sonatype.nexus.cfg` file, from the new instance you download. Go to <<config-http-port>> to identify the new 
repository manager port.

[[high-availability-enable]]
==== Enabling the Clusters

Follow these steps to enable the high availability feature:

. In your existing repository manager, open the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file.
. Remove the `#` before `nexus.clustered = true` to enable Hazelcast usage on start. 
. Go to the new repository manager and repeat steps 1 and 2, to enable the new instance.

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration synchronized with the high availability feature.

[[high-availability-startup]]
==== Starting the High Availability Service

At this point, you have already configured a new, distinct repository manager to run as a backup for the original 
instance. So, start both repository managers, by entering and executing `./bin/nexus run` in the command line. On 
startup you can review the log, in the command line. You should see connection messages via Hazelcast similar 
to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[high-availability-storage]]
==== Managing Nodes and Storage

With your two instances running, you can refer to the repository manager's user interface to view the nodes 
activated by the high availability feature. Go to the 'Nodes' screen under 'System' located in the 
'Administration' menu. Each machine should have a different node enabled as "true".
