[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability feature provides a more fail-safe repository manager, minimizing downtime, and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability. Usage of clusters sets the 
standard for automating the process of continuous uptime, relieving teams of manual recovery techniques, and 
creating a binary storage for the entire suite of features and components available in your repository managers.

The biggest draw to a high performing, well-synchronized backend is a self-maintaining system with improved 
disaster recovery features. Downtime can be costly to organizations that host a multitude of applications.
Therefore, an automated backup plan with distributed nodes decreases the chances of data loss. 

With backup support, you can use high availability methodologies to adopt load balancing and implement failover 
initiatives for physical and virtual servers alike. Currently, you can only synchronize two nodes. But down the 
line, you can expand the amount of clustered servers along your overall system backup plan.

[[how-high-availability]]
=== How is High Availability Implemented?

In {pro}, there are two ways can implement high availability. In both scenarios, you create a cluster with 
two distributed nodes. You can implement this by the following methods:

. Start with a fresh installation of 3.1 that includes the high availability feature.
. Add a second server with the high availability feature in place, connect it to your existing 3.1, then activate 
both.

High availability implementations are established between two nodes, joined together by a common blob store 
directory. The second node that you initiate is synchronized with the first. When configured, the nodes 
communicate with each other to create continuous uptime and the assurance that if one goes down, the second node 
can be used.

This feature is enabled across your nodes via link:https://hazelcast.com/[Hazelcast], which provides in-memory 
computing for active data and active backup. So, instead of making manual adjustments to your servers' 
infrastructure, processes, and tools, high availability via Hazelcast simplifies the configuration needed to 
maintain multiple repositories.

With active/active high availability in your two repository manager instances, you enable two interchangeable
nodes that act transparently as one instance. This makes them fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This node-driven awareness makes your clustered repository manager more scalable, allowing the nodes to 
automatically intercept component data between the two nodes.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

For both scenarios the minimum release to work with is 3.1. Depending on your current file system configuration, 
you may need to re-organize in preparation for high availability.

The repository manager must have the same network configuration as the second repository manager that you'll need 
to install, so the nodes can communicate. Enabling the second repository manager is the minimum requirement to 
allow redundancy between the nodes.

[[high-availability-servers]]
=== Supported Servers for High Availability

There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them across a mix of two:

- physical servers
- virtual machines
- Docker instances

You should be mindful of server performance, so your nodes can remain at their most optimal speed.

////
Begs the question what you need to do to enhance HA performance.
////

[[high-availability-start]]
=== Starting the Cluster for High Availability

Once you configure your file system to use high availability, where you share the blob store path and network 
configuration, follow the steps below to get started:

- <<high-availability-install,Install>> a fresh instance of {pro} that includes high availability, then 
duplicate the file. Alternatively you can add a second version of 3.1 with high availability to connect to
your existing version of 3.1.
- <<high-availability-enable,Activate the cluster>> from the file `org.sonatype.nexus.cfg`.
- <<high-availability-startup,Start>> and run the service after configuration.

[[high-availability-install]]
=== Installing {pro} for High Availability

To enable clustering for either a fresh installation, or existing repository manager with high availability, 
choose between the methods below.

==== Using a Fresh Installation of {pro}

To enable clustering for a fresh install of the repository manager, all you need to do is:

- Download {pro} 3.1.
- Duplicate the file.
- Set the location for the second repository manager so the nodes share same the blob store.

==== Adding a New Version of {pro} with High Availability

If you have an existing repository manager you can start high availability in a new instance to expose 
clustering, and connect the nodes for redundancy. So, this method assumes already use an existing version of 3.1. 
Follow these steps:

- Download a second version of {pro} 3.1.
- Set the location for the second repository manager in your file system so the nodes share same the blob store.

Regardless of method, be mindful that you shouldn't configure your nodes to share an entire data directory. For 
your repository manager to operate properly each node should have its own independent, non-conflicting OrientDB 
database. Instead, you only need to join the <<admin-repository-blobstores,blob store>> file locations together, 
where the nodes share access to components. 

For example, let's say you create two nodes, a default called "node-a" and a new node called "node-b". Then you 
designate the blob store directory for "node-a" at `sonatype-work/nexus3/data/blobs`. You must mount "node-b" to 
the same location.

TIP:: If you want to run repository manager instances on different hosts, to get the most out of high 
availability for uptime, you can use the same port.

[[high-availability-enable]]
==== Enabling the Nodes

Follow these steps to enable high availability:

. In the first repository manager, open the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file.
. Remove the `#` before +nexus.clustered=true+ to enable clustering at start-up. 
. Go to the second repository manager and repeat steps 1 and 2, to enable its server to use high availability, 
too.

Given that both repository managers are mounted to the same blob store, with the same path, synchronization of 
data and configuration will occur automatically.

////
Add section, expand on auto detection/multicast feature; see new ticket NEXUS-10918
////

[[high-availability-startup]]
==== Starting the High Availability Service

Bring the nodes to full service by starting the repository managers in the command line. As the console 
starts you can review the log, in the command line. You should see connection messages via Hazelcast similar to 
this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, the repository manager user interface allows you to see the contents of one node synchronized to the 
other.

To verify this connection, go to the 'Nodes' screen, under 'System' located in the 'Administration' menu. This 
screen provides details of the nodes in active/active high availability mode, where they are equal.