[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability feature provides a more fail-safe repository manager, minimizing downtime, and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability. Usage of clusters sets the 
standard for automating the process of continuous uptime, relieving teams of manual recovery techniques, and 
creating a binary storage for the entire suite of features and components available in your repository managers.

The biggest draw to a high performing, well-synchronized backend is a self-maintaining system with improved 
disaster recovery features. Downtime can be costly to organizations that host a multitude of applications.
Therefore, an automated backup plan with distributed nodes decreases the chances of data loss. 

With backup support, you can use high availability methodologies to adopt load balancing and implement failover 
initiatives for physical and virtual servers alike. Currently, you can only synchronize two nodes. But down the 
line, you can expand the amount of clustered servers along your overall system backup plan.

[[how-high-availability]]
=== How is High Availability Implemented?

High availability implementations are established between two nodes, joined together by a common blob store 
directory. The second node that you initiate is synchronized with the first. When configured, the nodes 
communicate with each other to create continuous uptime and the assurance that if one goes down, the second node 
can be used.

This feature is enabled across your nodes via link:https://hazelcast.com/[Hazelcast], which provides in-memory 
computing for active data and active backup. So, instead of making manual adjustments to your servers' 
infrastructure, processes, and tools, high availability via Hazelcast simplifies the configuration needed to 
maintain multiple repositories.

With active/active high availability in your two repository manager instances, you enable two interchangeable
nodes that act transparently as one instance. This makes them fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This node-driven awareness makes your clustered repository manager more scalable, allowing the nodes to 
automatically intercept component data between the two nodes.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

To make high availability work, you must have an existing repository manager already configured to start up. 
Depending on your current file system configuration, you may need to re-organize in preparation for high 
availability.

Then, ensure that your existing repository manager has the same network configuration as the second repository 
manager that you'll need to install, so the nodes can communicate. Enabling the second repository manager is the 
minimum requirement to allow redundancy between the nodes.

There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them across a mix of two:

- physical servers
- virtual machines
- Docker instances

You should be mindful of server performance, so your nodes can remain at their most optimal speed. Ultimately, 
you must ensure the two nodes utilized share a file system for blob storage and that they exist on the same 
network. So, enable your nodes for high availability follow these steps:

- <<high-availability-install,Create a second instance>> of {pro}.
- <<high-availability-enable,Activate the cluster>> from the file `org.sonatype.nexus.cfg`.
- <<high-availability-startup,Start>> and run the service after configuration.

[[high-availability-install]]
==== Create a New Binary

This step assumes you have an existing repository manager in place. Be mindful that you shouldn't configure your 
nodes to share an entire data directory. For your repository manager to operate properly, each node should have 
its own independent non-conflicting OrientDB database. Instead, you only need to join the 
<<admin-repository-blobstores,blob stores>> file locations together. All you need to do is:

- Duplicate the binary.
- Set the location for the second repository manager to the same location of the existing one, so the nodes share 
the blob store.

For example, let's say you create two nodes, a default called "node-a" and a new node called "node-b". Then you 
designate the blob store directory for "node-a" at `sonatype-work/nexus3/data/blobs`. You must mount "node-b" to 
the same location.

TIP:: If you want to run repository manager instances on different hosts, to get the most out of high 
availability for uptime, you can use the same port

[[high-availability-enable]]
==== Enabling the Nodes

Follow these steps to enable high availability:

. In your existing repository manager, open the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file.
. Remove the `#` before +nexus.clustered=true+ to enable clustering at start-up. 
. Go to the new repository manager and repeat steps 1 and 2, to enable the new instance.

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration synchronized with the high availability.
////
Add section, expand on auto detection/multicast feature
////

[[high-availability-startup]]
==== Starting the High Availability Service

At this point, you have already configured a new, distinct repository manager to run as a backup for the original 
instance. Bring the nodes to full service by starting the repository managers in the command line. As the console 
starts you can review the log, in the command line. You should see connection messages via Hazelcast similar to 
this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, the repository manager user interface allows you to see the contents of one node synchronized to the 
other.

To verify this connection, go to the 'Nodes' screen, under 'System' located in the 'Administration' menu. This 
screen provides details of the nodes in active/active high availability mode, where they are equal.