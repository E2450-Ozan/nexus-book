[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{pro}'s high availability feature is an integral part of the repository manager ability to scale. It reduces the 
complexity of deployment and infrastructure, while simultaneously improving user experience and performance. 

More specifically, this feature employs active/active high availability, which a provides performant backend with 
built-in support for enhanced security and license analysis for open source components.

Introducing high availability to your system sets the standard for a continuously operational application to 
reduce points of failure among content within your repository manager. Ultimately, this particular feature acts 
as a private binary cloud storage and backend for the entire suite of repository manager features and components 
utilized by globally distributed teams.

[[why-high-availability]]
=== Why Enable High Availability?

The high availability feature provides a storage backend for two {nxrm} instances co-located in the same data 
center. When it's enables users can use one version of {nxrm} with the assurance that if one of the nodes goes 
down a duplicated node can be used.

Instead of manually managing your infrastructure, processes, and tools associated with numerous repository 
manager installations, {pro} high availability feature provides a replicated storage system that eliminates 
complexity and offers a simple and cost effective way to provision, configure, manage, and maintain multiple 
repositories.

Active/active high availability is enabled across your nodes via Hazelcast. Hazelcast provides an interface to 
work with distributed data structures and other aspects of in-memory computing.

When enabling active/active high availability in your two repository manager instances, they act as 
individual nodes, fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This awareness also translates to increased efficiency, scalability, and control, allowing nodes to automatically 
choose the fastest route to fetch component data between the two nodes. With additional customization, these 
purpose-driven nodes can configure prefetching of components even before a client tool requests it, leading to an 
even higher level of enhanced performance.

[[high-availability-expectations]]
=== Planning High Availability for {nxrm}

{pro} is flexible, and can lend itself to a variety of different system architectures. So, before enabling this 
specific feature, determine which option is best for your existing architecture and system configuration before 
installing. There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them on two:

- separate physical servers
- virtual machines
- Docker instances

Ultimately, the rule of thumb is you ensure the two nodes utilized share a file system for blob storage and exist 
on the same network. Subsequently, the key goals for enabling high availability are as follows:

- <<high-availability-install,Installing a secondary version>> of {pro}.
- <<high-availability-configuration,Configuring the second binary>> as its own independent server.
- <<high-availability-enable,Activating Hazelcast>> from the file `org.sonatype.nexus.cfg`.
- <<high-availability-startup,Starting>> and <<high-availability-running,running>> the service upon activation.

[[high-availability-install]]
==== Installing a New Binary

Assuming you have an existing repository manager in place, download a brand new version of {pro}. 
Install this new binary in the same directory as your existing repository manager. You can refer to 
<<installation-archive>> or <<installation-installer>> for details on how to download and install.

As you install your new repository manager, be sure to mount it in the same directory as the existing repository 
manager.

[[high-availability-configuration]]
==== Configuring the Port of the Second Binary

In order for the two nodes to communicate, you must distinguish the `application-port` of your existing 
repository manager from the new instance you download. Go to <<config-http-port>> to identify the new repository 
manager port.


[[high-availability-enable]]
==== Enabling the High Availability Service
When you enable `nexus.clustered` in the configuration file, 

After you extract your newly downloaded `tar.gz` of {pro}, the follow these steps:

. In your existing repository manager, open the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file.
. Remove the `#` before `nexus.clustered = true` to enabl=le Hazelcast usage on start. 
. Go to the new repository manager and repeat steps 1 and 2, to enable the new instance.

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration synchronized with the high availability feature.

[[high-availability-startup]]
==== Starting the High Availability Service

At this point, you have already configured a new, distinct repository manager to run as a backup for the original 
instance. So, start both repository managers, by entering and executing `./bin/nexus run` in the command line. On 
startup you can review the log, in the command line. You should see connection messages via Hazelcast similar 
to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[high-availability-running]]
==== Running High Availability

As the console runs From the first enabled node default blob store directory is created after 
starting up the new repository manager

For example, if the first n is called "node-a" The node from your existing repository manager exposes the default 
blob store, when enabled in "node-a" is created...

[[high-availability-storage]]
==== Managing Nodes and Storage

Anytime you create a new blob store, make sure it's created from a location that is shared between two nodes. For 
example, if you are working with a NFS directory called...

You can use the <<admin-repository-blobstores,blob store>> screen to add the location of the shared blob store 
path. 

TIP:: Log in to your account and go to the 'Administration' menu. Click 'Nodes' under the 'System' category.  
You should see two nodes. Each machine should have a different node enabled as "true".
////
Assuming you have an already existing NXRM running with "nexus.clustered = true" enabled
After I download my second NXRM, the enable "nexus.clustered = true" on that instance, the synchronized node will 
appear on both instances in System > Nodes screen
////



IMPORTANT:: Configure your nodes to only share the blob store directory. Don't share the entire data directory. Each node should have its own independent 