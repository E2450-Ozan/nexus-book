[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

Using {pro}'s high availability feature is an integral part of the repository manager as it reduces the 
complexity of deployment and infrastructure, while simultaneously improving scalability, user experience, and 
performance. Active-active high availability allows you to create a performant backend with built-in support for 
active-active high availability configurations, enhanced security, and license analysis for open source 
components.

With high availability brought to light this feature sets the standard for a continuously operational application 
to reduce points of failure among content within your repository manager. Ultimately, this particular feature 
acts as a private binary cloud storage and backend for the entire suite of repository manager features and 
components utilized by globally distributed teams. 

[[why-high-availability]]
=== Why Enable High Availability?

Component Fabric is, essentially, a storage backend for multiple {nxrm} instances. When it's enables users can 
use one version of {nxrm} with the assurance that if one of the nodes goes down a duplicated node can be used. 
The intention of component fabric is that it can be used in one data center.

Component Fabric uses Hazelcast to enable active-active high availability across your network of nodes. Hazelcast
provides a convenient and familiar interface for developers to work with distributed data structures and other 
aspects of in-memory computing.
////
Compare and contrast traditional topology vs component fabric.
expand later.
Today organizations are using multiple repository manager installations in order to scale the required 
infrastructure and optimize the performance and availability of components for all consumers and producers.
////

Instead of manually managing your infrastructure, processes, and tools associated with numerous repository 
manager installations Component Fabric allows for a distributed and replicated storage system that eliminates 
complexity and offers a simple and cost effective way to provision, configure, manage, and maintain multiple 
repositories.

////
[[characteristics-component-fabric]]
=== Characteristics of Component Fabric

Section TBD

Expand upon use cases such as: High Availability a) Active/Active Load Balancing b) Managing Increased Load;
Disaster Recover - Node Failover, Disaster Recovery, Backup, Scaling Out
////

[[high-availability-configuration]]
=== Configuring High Availability

Enabling high availability automatically synchronizes component metadata between two nodes. This means 
you have to run at least two instances of {nxrm}. So, before you start configuring the feature consider these key 
expectation for managing your nodes in your repository manager instances:

* There are two nodes that need synchronization
* The two nodes are located the same data center
* Have an optimized network connection among the nodes
(Enabling Component Fabric assumes you aren't using a low-latency network)
* File system for the blob storage among all nodes is the same:: the file system storage must be on the same 
machines
////
(Before you configure Component Fabric, make sure the shared file system - how)
*Which is different from what you might encounter in a normal component fabric scenario, where you might have 
geographic diverse data centers. 
////

[[high-availability-performance]]
==== Node Optimization, Performance, and Tuning

The individual nodes are aware of their setup conditions such as:

* Memory availability and processor performance
* Network connectivity performance
* Characteristics of local storage in terms of size and performance

Nodes are connected to the component fabric and as such are aware of other nodes and can be configured to fulfill 
a specific purpose such as:

* High performance node in CI cluster 
* Slow backup node for potential disaster recovery
* Failover-node sitting in active standby mode
* Load-balanced node

The knowledge of the component fabric about deployment topology and performance allows nodes to choose the 
fastest route to fetch the component data from the fabric. The specified purpose of a node can configure 
pre-fetching of components from the fabric even before a client tool requests it as a performance enhancement.

////
move the section above to the intro
////

[[high-availability-install]]
=== High Availability Installation Options

{nxrm} installation consists of very few simple steps:

* Install binary on server
* Register and authenticate with the component fabric feature
* Declare minimal local node configuration such as storage and node performance (e.g. prefetch rules)

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration in the component fabric.

////
The section above implies once Hazelcast is enabled
////

Before starting Component Fabric determine which options is best for installation. There are three supported 
methods you can consider installation:

* on two separate physical servers
* on two virtual machines
* on two Docker instances

Assuming you have an existing repository manager in place and before starting your configuration, download a 
second binary of {pro} as explained in <<install-sect-downloading>>. Create another folder to prepare for a new 
binary that you will use for synchronization of component metadata from your existing repository manager.For 
example, you can create a directory called `nexus2`. However, you have not yet installed {pro}, review the 
installation steps from the beginning, as explained in <<install-introduction>>.

Extract the `tar.gz` from your newly downloaded version of {nxrm}, the follow these steps:

. In the source 
. In second folder (e.g. `nexus2`) go to the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file. 
. Change the `application-port={port-number}` in your first instance 
of {nxrm}. Then remove the `#` before `nexus.clustered = true`, enabling Hazelcast usage on start. Go to you the 
second `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file and repeat the same steps.

[[fabric-separate-servers]]
==== Installing on Separate Physical Servers

Section TBD

[[fabric-virtual-machines]]
==== Installing on Separate Virtual Machines

Section TBD

[[fabric-docker]]
==== Installing on Docker

Section TBD

////
Refer to https://docs.sonatype.com/display/~bradbeck/Demoing+HA+with+Docker as reference
////


[[high-availability-start]]
=== Enabling Hazelcast for High Availability

On startup in review your log. You should see connection messages via Hazelcast similar to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[after-high-availability-startup]]
=== After Start High Availability

TIP:: Log in to your account and go to the 'Administration' menu. Click 'Nodes' under the 'System' category.  
You should see two nodes. Each machine should have a different node enabled as "true".

////
Assuming you have an already existing NXRM running with "nexus.clustered = true" enabled
After I download my second NXRM, the enable "nexus.clustered = true" on that instance, the synchronized node will 
appear on both instances in System > Nodes screen
////