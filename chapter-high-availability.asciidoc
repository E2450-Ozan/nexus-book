[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

Using {pro}'s high availability feature is an integral part of the repository manager as it reduces the 
complexity of deployment and infrastructure, while simultaneously improving scalability, user experience, and 
performance. Active-active high availability allows you to create a performant backend with built-in support for 
active-active high availability configurations, enhanced security, and license analysis for open source 
components.

Introducing high availability to your system sets the standard for a continuously operational application to 
reduce points of failure among content within your repository manager. Ultimately, this particular feature acts 
as a private binary cloud storage and backend for the entire suite of repository manager features and components 
utilized by globally distributed teams.

[[why-high-availability]]
=== Why Enable High Availability?

Component Fabric is, essentially, a storage backend for multiple {nxrm} instances. When it's enables users can 
use one version of {nxrm} with the assurance that if one of the nodes goes down a duplicated node can be used. 
The intention of component fabric is that it can be used in one data center.

Active/active high availability is enabled across your nodes via Hazelcast. Hazelcast provides a convenient and 
familiar interface for you to work with distributed data structures and other aspects of in-memory computing.
////
Compare and contrast traditional topology vs component fabric.
expand later.
Today organizations are using multiple repository manager installations in order to scale the required 
infrastructure and optimize the performance and availability of components for all consumers and producers.
////

Instead of manually managing your infrastructure, processes, and tools associated with numerous repository 
manager installations Component Fabric allows for a distributed and replicated storage system that eliminates 
complexity and offers a simple and cost effective way to provision, configure, manage, and maintain multiple 
repositories.

////
[[characteristics-component-fabric]]
=== Characteristics of Component Fabric

Section TBD

Expand upon use cases such as: High Availability a) Active/Active Load Balancing b) Managing Increased Load;
Disaster Recover - Node Failover, Disaster Recovery, Backup, Scaling Out
////

[[high-availability-expectations]]
=== Planning High Availability for {nxrm}

Enabling high availability automatically synchronizes component metadata between two nodes, co-located in the 
same data center. This means the nodes can run independently as two separate instances, providing extra uptime 
within a data center. If one of the nodes goes down, you can use the other.

{pro} is flexible, and can lend itself to a variety of different system architectures. So, before enabling this 
specific feature, determine which option is best for your existing architecture and system configuration before 
installing. There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them:

- on two separate physical servers
- on two virtual machines
- on two Docker instances

Ultimately, the rule of thumb is you ensure the two nodes utilized share a file system for blob storage and exist 
on the same network. Subsequently, the key goals for enabling high availability are as follows:

- Create a plan to <<high-availability-nodes,manage your nodes>> for synchronization.
- Install a <<high-availability-install,secondary version>> of {pro}.
- <<high-availability-configuration,Configure the second binary>> as its own independent server
- Activate Hazelcast from the main configuration file

[[high-availability-nodes]]
=== Managing Nodes

Ultimately, each repository manager acts as an individual node, fully-aware of their setup parameters such as:

Memory availability and processor performance
Network connectivity performance
Characteristics of local storage in terms of size and performance

This awareness also translates to increased efficiency, scalability, and control, allowing nodes to automatically 
choose the fastest route to fetch component data from the fabric. With additional customization, the specified 
purpose of a node can configure prefetching of components from the fabric even before a client tool requests it, 
leading to an even higher level of enhanced performance.

[[high-availability-install]]
==== Installing a New Binary

Assuming you have an existing repository manager in place, you must download a brand new version of {pro}; refer 
to <<installation-archive>> or <<installation-installer>> for details on how to download and install.

[[high-availability-configuration]]
=== Configuring the Second Binary

After you download and install the new binary on a server you must configure that shares the same file system location as your existing repository manager.
////
* Register and authenticate with the component fabric feature
////
* Declare minimal local node configuration such as storage and node performance (e.g. prefetch rules)

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration in the component fabric.

////
The section above implies once Hazelcast is enabled
////


 and before starting your configuration, download a 
second binary of {pro} as explained in <<install-sect-downloading>>. Create another folder to prepare for a new 
binary that you will use for synchronization of component metadata from your existing repository manager.For 
example, you can create a directory called `nexus2`. However, you have not yet installed {pro}, review the 
installation steps from the beginning, as explained in <<install-introduction>>.

After you extract your newly downloaded `tar.gz` of {pro}, the follow these steps:

. In your existing repository manager go to the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file
. In second folder (e.g. `nexus2`) go to the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file. 
. Change the `application-port={port-number}` in your first instance 
of {nxrm}. Then remove the `#` before `nexus.clustered = true`, enabling Hazelcast usage on start. Go to you the 
second `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file and repeat the same steps.

[[fabric-separate-servers]]
==== Installing on Separate Physical Servers

Section TBD

[[fabric-virtual-machines]]
==== Installing on Separate Virtual Machines

Section TBD

[[fabric-docker]]
==== Installing on Docker

Section TBD

////
Refer to https://docs.sonatype.com/display/~bradbeck/Demoing+HA+with+Docker as reference
////


[[high-availability-start]]
=== Enabling Hazelcast for High Availability

On startup in review your log. You should see connection messages via Hazelcast similar to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[after-high-availability-startup]]
=== After Start High Availability

TIP:: Log in to your account and go to the 'Administration' menu. Click 'Nodes' under the 'System' category.  
You should see two nodes. Each machine should have a different node enabled as "true".

////
Assuming you have an already existing NXRM running with "nexus.clustered = true" enabled
After I download my second NXRM, the enable "nexus.clustered = true" on that instance, the synchronized node will 
appear on both instances in System > Nodes screen
////