[[high-availability]]
==  High Availability for {nxrm}
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability feature provides clustering for two separate instances of the repository manager. This 
is used to create shared storage for a more fail-safe backend in your repository manager, minimizing downtime and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability. Usage of clusters set the 
standard for automating the process of continuous uptime, relieving teams of manual recovery techniques, and 
creating a binary storage for the entire suite of features and components available in your repository managers.

[[why-high-availability]]
=== Why Cluster?

The biggest draw to a high performing, well-synchronized backend is a self-maintaining system with improved 
disaster recovery features. Downtime can be costly to organizations that host a multitude of applications.
Therefore, an automated backup plan with distributed clusters decrease the chances of data loss. 

With backup support, you can use high availability methodologies to resolve load balancing issues and implement
failover initiatives for physical and virtual servers alike. Currently, you can only synchronize two nodes. But 
down the line, you can expand the amount of clustered servers along your overall system backup plan.

[[how-high-availability]]
=== How is High Availability Implemented?

The high availability feature provides a storage backend for two {nxrm} instances colocated in the same data 
center.

High availability implementations are made among two nodes, joined together by a common blob store directory. The 
second node that you initiate is synchronized with the first. When configured, the nodes communicate with each 
other to provide uptime and the assurance that if one goes down a duplicated node can be used.

Instead of manual adjustments to your infrastructure, processes, and tools associated with your repository 
manager installations, the high availability feature provides a replicated storage system. This eliminates 
complexity and offers a simpler way to provision, configure, manage, and maintain multiple repositories.

This feature is enabled across your nodes via link:https://hazelcast.com/[Hazelcast]. Hazelcast provides an 
active/active approach to server clustering. It comes equipped with an interface for distributed data structures 
and other aspects of in-memory computing.

When enabling active/active high availability in your two repository manager instances, they act as individual 
nodes, fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This node-driven awareness within your clustered repository manager translates to increased efficiency, 
scalability, and control, allowing the nodes to automatically choose the fastest route to intercept component 
data between the two nodes. With additional customization, these purpose-driven nodes can configure prefetching 
of components even before a client tool requests it, leading to greater level of performance.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

{pro} is flexible and can lend itself to a variety of different system architectures. So, before enabling this 
specific feature, determine which option is best for your existing architecture and system configuration before 
installing. There are three supported infrastructures you can consider to facilitate high availability between 
your repository managers. You can install them across a mix of two:

- physical servers
- virtual machines
- Docker instances

You should to be mindful of server performance, so your repository manager nodes can remain at its most optimal 
speed. Ultimately, the rule of thumb is you ensure the two nodes utilized share a file system for blob storage 
and that they exist on the same network. Subsequently, to enable your nodes for high availability follow these 
steps:

- <<high-availability-install,Install a second instance>> of {pro}.
- <<high-availability-configuration,Configure the second binary>> as its own independent server.
- <<high-availability-enable,Activate the clusters>> from the file `org.sonatype.nexus.cfg`.
- <<high-availability-startup,Start>> and run the service after configuration.

[[high-availability-install]]
==== Installing a New Binary

Assuming you have an existing repository manager in place, download a brand new version of {pro}. 
Install this new binary in the same directory as your existing repository manager. You can refer to 
<<installation-archive>> for details on how to download and install.

As you install your new repository manager, be sure to mount it in the same directory as the existing repository 
manager. To properly configure a high-availability cluster the hosts in the cluster must all have access to the 
same shared storage. You need to share the nodes the directory location of your 
<<admin-repository-blobstores,blob store>>. You shouldn't configure your nodes to share an entire data directory. 
Each node should have its own independent non-conflicting OrientDB database.

For example, if you have two nodes -- a default called "node-a" and a new node called "node-b" -- and "node-a's"  
blob store directory located at `/foo/bar/blobs/foobar`. "Node-b" should be mounted to the same file system 
location.

[[high-availability-configuration]]
==== Configuring the Port of the Second Binary

After you extract your newly downloaded `tar.gz` of {pro}, you need to establish communication between your two 
nodes. To do this, you must distinguish the `application-port` of your existing repository manager's 
`org.sonatype.nexus.cfg` file, from the new instance you download. Go to <<config-http-port>> to identify the new 
repository manager port.

[[high-availability-enable]]
==== Enabling the Nodes

Follow these steps to enable the high availability feature:

. In your existing repository manager, open the `$NEXUS_HOME/etc/org.sonatype.nexus.cfg` file.
. Remove the `#` before `nexus.clustered = true` to enable Hazelcast usage on start. 
. Go to the new repository manager and repeat steps 1 and 2, to enable the new instance.

Any further work is done automatically by the interactions of the new repository manager with the data and 
configuration synchronized with the high availability feature.

[[high-availability-startup]]
==== Starting the High Availability Service

At this point, you have already configured a new, distinct repository manager to run as a backup for the original 
instance. So, start both repository managers, by entering and executing `./bin/nexus run` in the command line. On 
startup you can review the log, in the command line. You should see connection messages via Hazelcast similar 
to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, when you're log in as an administrator on one node, the other node automatically pairs the 
administrator to the second node. The {nxrm} interface allows you to see the nodes paired.

To verify this connection, go to the 'Nodes' screen, under 'System' located in the 'Administration' menu. This 
screen provides details of the node, detected as the backup server. 

For example, when you look at the row representing "node-a", you can see its 'UUID' (a unique identifier) 
as well as the label 'true'. The 'true' statement, in the 'Local' column, indicates the "node-b" is synchronized 
for backup. So, the moment you create and save a new repository called 'npm-internal' in "node-a", 'npm-internal' 
will appears in "node-b".

