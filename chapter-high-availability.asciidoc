[[high-availability]]
==  High Availability
{inrmonly}

[[high-availability-introduction]]
=== Introduction

High Availability (HA) for {nxrm} is designed to improve your repository manager's uptime. High availability
distributes traffic across a cluster of nodes, allowing horizontal scalability and reducing the need for a
single instance of {nxrm}.

This chapter guides you through the procedures to synchronize all repository data, system, and custom
configurations with the services provided by {nxrm}. Ultimately, the chapter explains how you can use {nxrm}
as a continuously operational, highly-available environment.

[[high-availability-benefits]]
=== Benefits of High Availability

{nxrm} HA is a tool for maintaining access to mission critical services even in the case of hardware failure.
If you desire high availability for your organization you should assess and evaluate your current system
architecture and configurations before optimizing your ecosystem with {nxrm} HA.

On the other hand, if you feel this feature meets your organization's needs, consider its benefits:

* prevention of data loss via node clustering
* consistent view of node health across all nodes
* active/active network architecture where all node have write-access
* mimimal uninterrupted use of HA servers

[[high-availability-requirements]]
=== Requirements for an HA Environment

Before you begin, you should have a basic understanding of the required tools and software to get your
HA environment configured. Hereâ€™s a list of the most important things you should have:

* An installation of {pro} HA
** If you have an existing {pro} installation without HA upgrade to an HA-supported version of {pro}, from the
instructions in the https://support.sonatype.com/hc/en-us/articles/231723267[support article]
* A load balancer, such as NGINX or Apache HTTP
* Shared storage, for maintaining repository data shared among multiple nodes

////
[[high-availability-node]]
==== What is a Node?

Each instance of {nxrm} in an HA cluster is a node.


They are synchronized inside a cluster via a service called Hazelcast which provides the infrastructure for
replicated data. Hazelcast can employ multicast to discover
participating nodes though it can support node sychronization in other ways such as <<high-availability-aws,AWS>>
also covered in this chapter.
////

[[high-availability-prepare]]
==== Preparing an HA Environment

A minimum of three nodes is required to form an HA cluster. This allows nodes to vote on each change that passes
through the cluster, and reduces the probability of node failure. As components are uploaded or modified, metadata
about those components are synchronized across the cluster. Component binaries sit in a shared, on-disk blob store
accessible to all three nodes.

Nodes can run on physical or virtual servers, Docker containers, or cloud services like <<high-availability-aws,AWS>>.
Your network configuration may consist of at least three different operating system instances, or perhaps even
different virtual operating system instances spread across different physical machines.

Whichever combination of servers you choose, each node must be visible to each other so the cluster can form.
In the cluster itself, the nodes must share the same network so that latency doesn't negatively impact
performance.

[[high-availability-diagram]]
==== Example of an HA Environment

This is an example of {nxrm} HA's key layers. The diagram shows the discovery of enabled nodes via Hazelcast
and distribution of traffic passing through a load balancer to the nodes.

[[fig-high-availability-architecture]]
.Sample High Availability Environment
image::figs/web/high-availability-architecture[scale=60]

As stated in <<high-availability-storage>>, you must to provide your own shared storage to host shared data,
such as blob stores and exported database content.

[[high-availability-network]]
==== Supported Network Architecture

{nxrm} HA supports a three-node active/active setup. Unlike an active/passive architecture where only one
node allow writes or active/standby where a backup stays offline until needed, {nxrm} HA permits write-access
to all nodes.

[[high-availability-replication]]
==== What Gets Replicated?

Component binaries are stored as blobs, in a user-specified blob store location. As mentioned in
<<high-availability-blob-store>> you will need to set up an external, shared blob store location accessible
to all three nodes. After configuration, all repositories and data gets synchronized across the nodes in the
cluster.

Everything else is replicated through the embedded, clustered database, OrientDB. This includes component metadata,
as well as system data and configuration.

NOTE: Some repository manager-level modifications won't synchronize across nodes. See <<high-availability-environment >>
for details on what's needed to properly configure an individual node for HA.

[[high-availability-system]]
==== Making a Cluster Highly Available

An HA cluster is established when nodes communicate with each other and distribute the load of repository
data and system configuration equally among themselves. Still, you must prepare your HA environment to mitigate
any single points of failure. A load balancer, therefore, improves the distribution of workload across clusters
in your HA environment. If one node goes down it redirects traffic to all surviving nodes.

[[high-availability-connection]]
==== Monitoring Node Connections

In the event a single node loses connection to the cluster, the remaining nodes will continue to make decisions
on which data changes are valid. The disconnected node will reject further writes until it rejoins the cluster.

From the user interface the most accessible way to view active nodes is from the 'Nodes' screen, in the
'Adminstration' menu. It displays all clustered nodes in a table. In this table, notice a distinguishable node
listed as 'true'. The 'true' value indicates that you're viewing your local node from the client (e.g. browser).
Conversely, all values listed as 'false' imply the additional, synchronized nodes are configured to their
own servers.

[[high-availability-storage]]
==== Storing Critical Data

NOTE: As a best practice, use a remote external server to store your exported databases, blob stores, data
directory, and any other custom configurations within the {nxrm} HA cluster.

The core of an HA environment is to have reliable, fault-tolerant storage. The solution is to utilize a shared
storage network to maintain repository data and configurations. To do this:

* create a storage location for each node, including their data directories and exported databases
* create a storage location where all nodes can shared component binary information as a common blob store access
point.

[[high-availability-methods]]
=== Methods to Configure a Cluster

When you're ready to set up a cluster, consider these two methods:

* If you're setting up a new cluster, skip to <<high-availability-new,Using a Fresh Installation of {pro}>>.
* If you're converting an existing server into a cluster, skip to <<high-availability-existing,Converting an
Existing Instance of {pro} into a Cluster>>.

When the cluster is formed you can <<high-availability-verify,verify node synchronization>> from the 'Nodes'
screen located in 'System' from the 'Administration' menu.

[[high-availability-new]]
==== Using a Fresh Installation of {pro}

To set up a fresh installation of {pro} for high availability:

1. Download {pro}, supported with high availability.
2. Install two additional {pro} instances on different hosts to establish three nodes.
3. Configure the blob store in the first node to an external location where the second and third nodes can
access it.

[[high-availability-existing]]
==== Converting an Existing Instance of {pro} into a Cluster

If you have an existing version pre-HA installation of {pro} 3, you can convert it into a cluster. Follow the
steps below to synchronize it with the second and third nodes.

1. Stop running the existing repository manager.
2. Follow the steps in the https://support.sonatype.com/hc/en-us/articles/231723267[support article] to
upgrade the repository manager to a version that supports high availability.
3. Download and unpack a new repository manager to establish a second node.
4. Create a third instance by copying the second repository manager to establish a third node.
5. Configure the blob store in the first node to an external location where the second and third can access it.
6. <<high-availability-nodes,Enable>> high availability on the three nodes.
7. Start the first node and wait for its start-up sequence to complete, then start the second and third
nodes to form the cluster.

[[high-availability-move]]
==== Moving Blob Stores from an Existing Instance of {pro}

As inferred in <<high-availability-storage>>, your node may contain blob stores created before initiating HA.
As is, those blob stores and their data will not be replicated. So to preserve them with data intact, you must
relocate them to the shared location planned for your HA environment. To do so, follow the steps in the
https://support.sonatype.com/hc/en-us/articles/235816228[support article]. After completing the steps, refer
to step 3 in <<high-availability-existing>> to complete HA set up.

[[high-availability-blob-store]]
==== Configuring a Node to Share a Blob Store

NOTE: Configuring a single node to share an entire `sonatype-work` directory will undermine HA configuration,
and might cause functional errors in the cluster. As mentioned in <<high-availability-storage >>, configure
the nodes to share access to components, instead.

To configure a single node sharing blob store access among new nodes:

1. Create a directory in an external location.
2. Start your primary repository manager.
3. Choose a 'Name' and add a 'Path' from the 'Blob stores' screen, referencing the new directory you created.
4. Click 'Create blob store'.

After the shared storage for blob stores is set up, continue to point all new repositories you create to the
shared location.

*Example: Configuring a Shared Blob Store for a Cluster*

Let's say you create a new blob store in an external backup location (e.g. `data-location`) and you want
to point a hosted npm repository to this location, for shared blob store access. Do the following:

1. Select a recipe from the 'Repositories' form, i.e. 'npm (hosted)'
2. Pick the `data-location` blob store in the 'Storage' section of the 'Repositories' form.
3. Click 'Create repository' to establish the new repository.

[[high-availability-nodes]]
==== Enabling High Availability

CAUTION: In the event you have empty nodes and are adding existing configured nodes to it, the existing 
unconfigured nodes would erase the existing configuration of the nodes added. When creating a cluster, it is 
important you start the configured nodes before the empty nodes to avoid unwanted configuration loss.

When you enable high availability, the nodes discover one another via link:https://hazelcast.com/[Hazelcast].
Hazelcast, by default, employs multicast to discover cluster members, but it supports node discovery in other
ways. If the default configuration isn't suitable for your network infrastructure, you will need to customize
`$install-dir/etc/fabric/hazelcast.xml`. See <<high-availability-aws>> for a concrete example.

Follow these steps to enable high availability:

1. In the first repository manager, open the `$data-dir/etc/nexus.properties` file.
2. Remove the `#` before +nexus.clustered = true+ to enable the node at start-up. 
3. Go to the second and third repository managers and repeat steps 1 and 2, to enable them for high availability.

[[high-availability-startup]]
==== Startup and Confirming Node Connectivity

After enabling high availability for your nodes, check the console to confirm that multicast discovers all three
corresponding nodes.

When you start the nodes, you will see a message in the `nexus.log` confirming the connection of the cluster
members, like the one below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [3] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702
    Member [192.168.99.1]:5703 this
}
----

[[high-availability-verify]]
==== Verifying Synchronization

At runtime, the repository manager user interface allows you to view the status of the nodes, regardless of
which you connect to, as they are synchronized.

See <<nodes>> for details on viewing active nodes in a cluster.

[[high-availability-environment]]
==== Configuring a Cluster after Setup

Once you have your high availability environment set up, be aware that almost all configuration done via the 
user interface is shared among all nodes in the cluster. In an HA cluster all nodes are treated equally.
For example, if you create a new repository all nodes in the cluster will be able to see it and utilize it.
Or if you want to change your 'Email Server' port you just need to do it once via the user interface on any
of the servers and the change will share.

NOTE: Same as a single server be aware, if multiple people are configuring something at the same time in your 
cluster, it may appear the changes are not sharing. If you refresh your screen, the latest changes will appear.

There are some things, however, that are not done or shared within the UI and need to be done on each individual 
server. These include:

- Any configuration files you add or modify (such as specifying a port via `nexus.properties` or setting up SSL)
- 'Refresh Interval' of the UI 'Log Viewer' setting
- Most log messages are not shared across the server, however logging levels are shared
- 'Metrics' displayed are for the individual server
- A 'Support ZIP' is for the individual server. If you have issues forming a cluster, consult your support
technician and provide support zips for all nodes.

TIP: Scheduled tasks will run against one node unless the 'Multi node' configuration option is selected or the 
task affects something that is in itself shared (like compaction of blob stores).

Regardless, {nxrm} configuration should not be done through the cluster's load balancer. Configuration should
occur on the individual node level.

When adding new nodes to the existing cluster be aware that they will get the shared configuration of the cluster 
regardless of how they are preconfigured.

[[high-availability-aws]]
==== Configuring High Availability for Amazon Web Services

{nxrm} can be deployed on cloud-computing services, such as Amazon Web Services (AWS). Depending on your network
security, additional configuration may be required. For example, if you use a network layer firewall application
it may block multicast communication. If such a failure occurs you will need to modify the Hazelcast configuration
file.

To configure Hazelcast for automatic node discovery find the `<join>` tag in `$install-dir/etc/fabric/hazelcast.xml`.
Then, edit the file for each node:

1. Change the value in `<multicast enabled="true">` to `"false"`.
2. Change the value in `<aws enabled="false">` to `"true"`.
3. Save the file.
4. Reboot each node in the cluster.

The `$install-dir/etc/fabric/hazelcast.xml` file with the modified properties will look similar to this:
----
<join>
    <multicast enabled="false">
       <multicast-group>224.2.2.3</multicast-group>
       <multicast-port>54327</multicast-port>
    </multicast>
    <tcp-ip enabled="false">
        <interface>127.0.0.1</interface>
    </tcp-ip>
    <aws enabled="true">
        <access-key>my-access-key</access-key>
        <secret-key>my-secret-key</secret-key>
        <!--optional, default is us-east-1 -->
        <region>us-west-1</region>
        <!--optional, default is ec2.amazonaws.com. If set, region shouldn't be set as it will override this property -->
        <host-header>ec2.amazonaws.com</host-header>
        <!-- optional, only instances belonging to this group will be discovered, default will try all running instances -->
        <security-group-name>security-group-name</security-group-name>
        <tag-key>type</tag-key>
        <tag-value>nexus-nodes</tag-value>
    </aws>
</join>
----

[[high-availability-backup]]
==== Backing up your HA Cluster

NOTE: The task described here only backs up the configuration and metadata.  The (shared) blob store(s) must be 
backed up independently.

Backup for HA uses the same concepts as outlined in <<backup>>. The notable exception is that when creating your 
'Export configuration & metadata for backup' scheduled task you must choose a node for the backup to run against. 
Since these nodes are sharing the same data, in most cases your selection should not matter.

Running the same scheduled task against the same node over and over does provide assurance that your files are 
backed up in the same place continuously.

While the scheduled task runs, the node configured against becomes read only. During this time, the other nodes 
in the cluster function as normal. When the backup is complete, the node picks up any changes made to the cluster 
configuration while it was performing the task. Any write operations run specifically against this node (as 
opposed to the cluster) will be run at that time as well.

[[high-availability-restore]]
==== Restoring your HA Cluster

Similar to the concepts outlined in <<backup>>, you can restore your a node with shared data in your HA environment.
You are required to choose a node from which you desire to restore OrientDB database contents. Then, follow the steps
in detail from <<backup-restore>>.
