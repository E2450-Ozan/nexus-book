[[high-availability]]
==  High Availability
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability feature provides a more fail-safe repository manager, minimizing downtime, and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability. Usage of clusters sets the 
standard for automating the process of continuous uptime, relieving teams of manual recovery techniques, and 
creating a binary storage for the entire suite of features and components available in your repository managers.

The biggest draw to a high performing, well-synchronized backend is a self-maintaining system with improved 
disaster recovery features. Downtime can be costly to organizations that host a multitude of applications.
Therefore, an automated backup plan with distributed nodes decreases the chances of data loss. 

With backup support, you can use high availability methodologies to adopt load balancing and implement failover 
initiatives for physical and virtual servers alike. Currently, you can only synchronize two nodes. But down the 
line, you can expand the amount of clustered servers along your overall system backup plan.

[[how-high-availability]]
=== How is High Availability Implemented?

In {pro}, there are two ways you can implement high availability. In both scenarios, you create a cluster with 
two distributed nodes. You can implement this by the following methods:

. Start with a fresh installation of {pro} that includes the high availability feature.
. Enable high availability in version {pro} 3 to synchronize content from a 2.x to 3.y upgrade. 

IMPORTANT:: To upgrade from 2.x to 3.y, review <<upgrade-version-two-three>> before configuring high availability.
////
. Add a second server with the high availability feature in place, then configure it in your existing instance of 
{pro}
////

High availability implementations are established between two nodes, joined together by a common blob store 
directory. The second node that you initiate is synchronized with the first. When configured, the nodes 
communicate with each other to create continuous uptime and the assurance that if one goes down, the second node 
can be used.

With active/active high availability in your two repository manager instances, you enable two interchangeable
nodes that act transparently as one instance. This makes them fully-aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This node-driven awareness makes your clustered repository manager more scalable, allowing the nodes to 
automatically intercept component data between the two nodes.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

Depending on your current file system configuration, you may need to re-organize in preparation for high 
availability.

////
How would we tell them to re-organize their file system config for HA, if at all?
////

The repository manager must have the same network configuration as the second repository manager that you'll need 
to install, so the nodes can communicate. Enabling the second repository manager with high availability is the 
minimum requirement to allow redundancy between the nodes.

[[high-availability-servers]]
=== Supported Servers for High Availability

There are three supported infrastructures you can consider to facilitate high availability between your 
repository managers. You can install them across a mix of two:

- physical servers
- virtual machines
- Docker instances

////
Begs the question what you need to do to enhance HA performance.
////

[[high-availability-prepare]]
=== Preparing a Cluster for High Availability

When you have a workable file structure, where your two blob store file paths are the same, follow the steps 
below to get started:

- <<high-availability-install,Install>> a fresh instance of {pro} that includes high availability, then 
duplicate the file.
- <<high-availability-configure,Configure and activate>> the cluster.
- <<high-availability-startup,Start>> the service.

[[high-availability-install]]
=== Installing {pro} for High Availability

To enable clustering from a fresh installation of {pro}, follow the steps below.

==== Using a Fresh Installation of {pro}

To prepare a fresh installation for {pro} for high availability

- Download {pro}
- Duplicate the {pro} file, creating a second folder
- Set the location for the second repository manager so the nodes share same the 
<<admin-repository-blobstores,blob store>> path

For example, let's say you create two nodes, a default called "node-a" and a new node called "node-b". Then you 
designate the blob store directory for "node-a" at `sonatype-work/nexus3/blobs`. You must mount "node-b" to 
the same location.

////
==== Adding a New Version of {pro} with High Availability

If you have an existing repository manager you can start high availability in a new instance to expose 
clustering, and connect the nodes for redundancy. So, this method assumes you already use an existing version of 
{pro} with high availability. Follow these steps:

- Download a second version of {pro}.
- Set the location for the second repository manager in your file system so the nodes share same the blob store.

From Benjamin: Somewhere we should also mention that in case of an existing NX instance, that instance needs to 
be rebooted first. The first node to join/form the cluster defines the current NX config, any other node joining 
afterwards will copy that config. I.e. if one was to start the second NX instance first, all config 
from the existing instance gets lost when that would join the cluster later.
////
Regardless of method, be mindful that you shouldn't configure your nodes to share an entire data directory. For 
your repository manager to operate properly each node should have its own independent, non-conflicting OrientDB 
database. Instead, you only need to join the <<admin-repository-blobstores,blob store>> file locations together, 
where the nodes share access to components.

////
TIP:: If you run repository manager instances on different hosts, to get the most out of high availability for 
uptime, you can use the same port.

From Joe: It is unclear to me if getting the most of HA involves running on different hosts or using the same 
port when running on different hosts. I was going to suggest striking "want to" then I realized maybe different 
hosts performs better. So comment instead of suggestion.
////

[[high-availability-configure]]
==== Enabling the Nodes

Follow these steps to enable high availability:

. In the first repository manager, open the `$data-dir/etc/nexus.properties` file.
. Remove the `#` before +nexus.clustered = true+ to enable the node at start-up. 
. Go to the second repository manager and repeat steps 1 and 2, to enable it to use high availability, too.

Given that both repository managers are mounted to the same blob store, with the same path, synchronization of 
data and configuration will occur automatically.

[[high-availability-startup]]
==== Starting the High Availability Service

Bring the nodes to full service by starting the repository managers in the command line. As the console 
starts you can review the log. You will see connection messages similar to this, below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [2] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702 this
}
----

The 'Members' from the terminal output represent the nodes that automatically get discovered via multicast at startup.
The nodes are synchronized via link:https://hazelcast.com/[Hazelcast], which provides in-memory computing for active
data and active backup.

Hazelcast can rely on multicasting to discover HA-enabled nodes, and open communication among them. So, instead of
making manual adjustments to your servers' infrastructure, system configurations, and tools, Hazelcast may simplifies
the configuration needed to maintain nodes in an HA cluster.

NOTE::  Node-listening via Hazelcast may not work with default activation of high availabilty. In some cases advanced
configuration of the `hazelcast.xml` file may be needed for multicasting to persist.

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, the repository manager user interface allows you to see the contents of one node synchronized to the 
other.

To verify this connection, go to the 'Nodes' screen, under 'System' located in the 'Administration' menu. This 
screen provides details of the nodes in active/active high availability mode, where they are equal.

=== Configuration of {nxrm} within an HA environment

Once you have your high availability environment set up, be aware that almost all configuration done via the UI 
is shared between all nodes in the cluster. There is no master node you must hit; they are all treated equally. 
For example, if you create a new repository all nodes in the cluster will be able to see it and utilize it. Or 
if you want to change your 'Email Server' port you just need to do it once via the UI on any of the servers and 
the change will share. Because all servers share out the changes, changing on any is acceptable.

NOTE: Same as a single server be aware, if multiple people are configuring something at the same time in your 
cluster, it may appear the changes are not sharing. If you refresh your screen, you will see the changes when 
they come across.

There are some things, however, that are not done or shared within the UI and need to be done on each individual 
server. These are:

- Any CLI configurations you do (such as specifying a port via nexus.properties or setting up SSL)
- 'Refresh Interval' of the GUI 'Log Viewer' setting
- Most log messages are not shared across the server (some few are) however logging levels are shared
- 'Metrics' displayed are for the individual server (and not for the cluster)
- A 'Support ZIP' is for the individual server. Consult with your support technician which zips they need if 
troubleshooting.
- 'Analytics' events are per server
- 'Audit' events are per server
////
last 2 should be changing with NEXUS-10489
////

TIP: Scheduled tasks will run against one node unless the 'Multi node' configuration option is selected or the 
task affects something that is in itself shared (like compaction of blob stores).

Regardless, {nxrm} configuration is not done via any load balancers that might be in place. It is done on the 
individual node level and shared or not.

When adding new nodes to the existing cluster be aware that they will get the shared configuration of the cluster 
regardless of how they are preconfigured.

CAUTION: In the event you have empty nodes and are adding existing configured nodes to it, the existing 
unconfigured nodes would erase the existing configuration of the nodes added. When creating a cluster, it is 
important you start the configured nodes before the empty nodes to avoid unwanted configuration loss.

=== Configuring HA for Cloud Services

By default multicast via Hazelcast opens communication between HA-enabled nodes. {nxrm} can be deployed on cloud-
computing services, such as AWS, but tepending on your network security, additional configuration may be required.
For example, if you use a network layer firewall application it may block multicast communication. If such a
failure occurs you will need to access `hazelcast.xml`. To configure Hazelcast for automatic discovery on secured
networks, locate `hazelcast.xml` in `$install-dir/etc/fabric/`. Then:

1. Locate the `<join>` tag
2. Edit the value in `<multicast enabled="true">` to `"false"`
3. Edit the value in `<aws enabled="false">` to `"true"`
4. Save the file.
5. Reboot the repository manager.

The `$install-dir/etc/fabric/hazelcast.xml` with the modified properties will look like this:
----
<join>
  <multicast enabled="true">
    <multicast-group>224.2.2.3</multicast-group>
    <multicast-port>54327</multicast-port>
  </multicast>
  <tcp-ip enabled="false">
   <interface>127.0.0.1</interface>
  </tcp-ip>
     <aws enabled="false">
       <access-key>my-access-key</access-key>
       <secret-key>my-secret-key</secret-key>
       <!--optional, default is us-east-1 -->
       <region>us-west-1</region>
       <!--optional, default is ec2.amazonaws.com. If set, region shouldn't be set as it will override this property -->
       <host-header>ec2.amazonaws.com</host-header>
       <!-- optional, only instances belonging to this group will be discovered, default will try all running instances -->
       <security-group-name>hazelcast-sg</security-group-name>
       <tag-key>type</tag-key>
       <tag-value>hz-nodes</tag-value>
   </aws>
</join>
----

In this instance, the revised file will establish participation among HA-enable nodes within a cluster. The terminal
output mentioned in <<high-availability-startup>> validates the connection.


