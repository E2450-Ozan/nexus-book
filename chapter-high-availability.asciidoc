[[high-availability]]
==  High Availability
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability feature provides a more fail-safe repository manager, minimizing downtime, and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability clusters. Clustering sets the 
standard for automating the process of continuous uptime, relieving teams of manual recovery techniques, and 
creating a binary storage for the entire suite of features and components available in your repository managers.

[[how-high-availability]]
=== How is High Availability Implemented?

In {pro}, there are two ways you can implement high availability. In the scenarios below you create a cluster
with three distributed nodes. You can implement this by the following methods:

* Start with a fresh installation of {pro} that includes high availability.
* Upgrade an older version of {pro} 3 to a version that supports high availability.

////
. Add a second server with the high availability feature in place, then configure it in your existing instance of 
{pro}
////

High availability clustering is established between three nodes, joined together by a common blob store path.
The second and third nodes that you initiate are synchronized with the first. When configured, the nodes communicate
with each other to create continuous uptime and the assurance that if one fails, the other nodes can be used.

When you enable high availability among the nodes it creates a cluster of three interchangeable nodes that act
transparently as one instance. This makes them fully aware of their setup parameters such as:

- Memory availability and processor performance
- Network connectivity performance
- Characteristics of local storage in terms of size and performance

This node-driven awareness makes your clustered repository manager more scalable, allowing the nodes to automatically
intercept component data.

[[high-availability-expectations]]
=== Requirements for Clustering in {pro}

Depending on your current file system configuration, you may need to re-organize in preparation for clustering.

Each repository manager acts as a node and all nodes must share the same network configuration as the first
repository, so the nodes can communicate. Enabling three repository manager instances with high availability is
the minimum requirement to allow redundancy between the nodes.

////
Re-write intro and add requirements as bullets. Add a blob store config example, then merge this section into How to
Implement HA, above
////

[[high-availability-servers]]
=== Supported Servers for High Availability

There are three supported infrastructures you can consider to facilitate high availability among your repository
managers. You can install them across a mix of the following environments:

- physical servers
- virtual machines
- Docker instances

[[high-availability-prepare]]
=== Preparing a Cluster for High Availability

When you configure all blob store file paths to be the same, follow the steps below to get started:

- <<high-availability-install,Install>> a fresh instance of {pro}.
- <<high-availability-configure,Configure and activate>> the cluster.
- <<high-availability-startup,Start>> the service.

[[high-availability-install]]
=== Installing {pro} for High Availability

Whether you use a fresh installation or upgrade a non-HA-supported version of {pro} to form a cluster, you
need to run the additional nodes on different hosts. Follow the appropriate scenario in this section. 

==== Using a Fresh Installation of {pro}

To prepare a fresh installation of {pro} for high availability

1. Download {pro}, supported with high availability.
2. Install two additional {pro} instances on different hosts to establish three nodes.
3. Configure all three nodes to use the same <<admin-repository-blobstores,blob store>> path.

==== Converting an Existing Instance of {pro} into a Cluster

If you have an earlier version of {pro} 3, you can convert it into a cluster. Follow the steps below to
configure this non-HA-supported version so it's synchronized with the second and third nodes.

1. Stop running the existing repository manager.
2. Follow the steps in the https://support.sonatype.com/hc/en-us/articles/231723267[support article] to
upgrade the repository manager to a version that supports high availability.
3. Download and unpack a new repository manager to establish a second node.
4. Create a third instance by copying the second repository manager to establish a third node.
5. <<high-availability-configure,Enable>> the first, newly upgraded node for high availability. Then enable
high availability for the second and third nodes.
6. Configure a location for all three nodes to share the same the blob store.
7. Start the first node and wait for its start-up sequence to complete, then start the second and third
nodes to form the cluster.

In both scenarios the data from the first node is synchronized with the second and third nodes, so that if
one node fails the other nodes in the cluster will maintain that data.

[[high-availability-blob-store]]
=== Configuring Nodes to Share the Same Blob Store

In order to synchronize data each node should have its own independent, non-conflicting OrientDB database.
Instead, you only need to join the <<admin-repository-blobstores,blob store>> file locations together, where
the nodes share access to components.

Regardless of method, be mindful that you shouldn't configure your nodes to share an entire data directory.

For example, let's say you create three nodes, a default called `/foo/data/node-a` and two new nodes called
`/foo/data/node-b` and `/foo/data/node-c`, respectively. Then you designate the blob store directory for
`foo/data/node-a` at `sonatype-work/nexus3/blobs`. You must mount `/foo/data/node-b` and `/foo/data/node-c`
to the same location.

[[high-availability-configure]]
=== Enabling the Nodes

Follow these steps to enable high availability:

1. In the first repository manager, open the `$data-dir/etc/nexus.properties` file.
2. Remove the `#` before +nexus.clustered = true+ to enable the node at start-up. 
3. Go to the second and third repository managers and repeat steps 1 and 2, to enable them for high availability.

Given that all nodes are mounted to the same blob store, with the same path, synchronization of data and configuration
will occur automatically.

////
the phrasing in bullet 2 above will likely be different, hence this note to myself
////

[[high-availability-startup]]
==== Starting the High Availability Service

Bring the nodes to full service by starting the repository managers. You will see connection messages similar to
this:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [3] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702
    Member [192.168.99.1]:5703 this
}
----

The nodes are synchronized via link:https://hazelcast.com/[Hazelcast], which provides in-memory computing for
active data and active backup. Hazelcast can employ multicast to discover cluster members, but it supports node
discovery in other ways. If the default configuration isn't suitable for your network infrastructure, you will
need to customize `$install-dir/etc/fabric/hazelcast.xml`. See <<high-availability-aws>> for a concrete example.

[[high-availability-environment]]
=== Configuring {pro} Environment for High Availability

Once you have your high availability environment set up, be aware that almost all configuration done via the UI 
is shared between all nodes in the cluster. There is no master node you must hit; they are all treated equally. 
For example, if you create a new repository all nodes in the cluster will be able to see it and utilize it. Or 
if you want to change your 'Email Server' port you just need to do it once via the UI on any of the servers and 
the change will share. Because all servers share out the changes, changing on any is acceptable.

NOTE: Same as a single server be aware, if multiple people are configuring something at the same time in your 
cluster, it may appear the changes are not sharing. If you refresh your screen, you will see the changes when 
they come across.

There are some things, however, that are not done or shared within the UI and need to be done on each individual 
server. These are:

- Any CLI configurations you do (such as specifying a port via nexus.properties or setting up SSL)
- 'Refresh Interval' of the GUI 'Log Viewer' setting
- Most log messages are not shared across the server (some few are) however logging levels are shared
- 'Metrics' displayed are for the individual server (and not for the cluster)
- A 'Support ZIP' is for the individual server. Consult with your support technician which zips they need if 
troubleshooting.
- 'Analytics' events are per server
- 'Audit' events are per server
////
last 2 should be changing with NEXUS-10489
////

TIP: Scheduled tasks will run against one node unless the 'Multi node' configuration option is selected or the 
task affects something that is in itself shared (like compaction of blob stores).

Regardless, {nxrm} configuration is not done via any load balancers that might be in place. It is done on the 
individual node level and shared or not.

When adding new nodes to the existing cluster be aware that they will get the shared configuration of the cluster 
regardless of how they are preconfigured.

CAUTION: In the event you have empty nodes and are adding existing configured nodes to it, the existing 
unconfigured nodes would erase the existing configuration of the nodes added. When creating a cluster, it is 
important you start the configured nodes before the empty nodes to avoid unwanted configuration loss.

[[high-availability-aws]]
=== Configuring High Availability for Amazon Web Services

{nxrm} can be deployed on cloud-computing services, such as Amazon Web Services (AWS). Depending on your network
security, additional configuration may be required. For example, if you use a network layer firewall application
it may block multicast communication. If such a failure occurs you will need to modify the Hazelcast configuration
file.

To configure Hazelcast for automatic node discovery find the `<join>` tag in `$install-dir/etc/fabric/hazelcast.xml`.
Then, edit the file for each node:

1. Change the value in `<multicast enabled="true">` to `"false"`.
2. Change the value in `<aws enabled="false">` to `"true"`.
3. Save the file.
4. Reboot each node in the cluster.

The `$install-dir/etc/fabric/hazelcast.xml` file with the modified properties will look similar to this:
----
<join>
    <multicast enabled="false">
       <multicast-group>224.2.2.3</multicast-group>
       <multicast-port>54327</multicast-port>
    </multicast>
    <tcp-ip enabled="false">
        <interface>127.0.0.1</interface>
    </tcp-ip>
    <aws enabled="true">
        <access-key>my-access-key</access-key>
        <secret-key>my-secret-key</secret-key>
        <!--optional, default is us-east-1 -->
        <region>us-west-1</region>
        <!--optional, default is ec2.amazonaws.com. If set, region shouldn't be set as it will override this property -->
        <host-header>ec2.amazonaws.com</host-header>
        <!-- optional, only instances belonging to this group will be discovered, default will try all running instances -->
        <security-group-name>security-group-name</security-group-name>
        <tag-key>type</tag-key>
        <tag-value>nexus-nodes</tag-value>
    </aws>
</join>
----

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, the repository manager user interface allows you to see the contents of one node synchronized with the others.
See <<nodes>> for details on viewing active nodes in cluster.
