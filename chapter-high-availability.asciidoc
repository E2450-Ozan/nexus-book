[[high-availability]]
==  High Availability
{inrmonly}

[[high-availability-introduction]]
=== Introduction

{nxrm}'s high availability (HA) feature provides a more fail-safe repository manager, minimizing downtime, and 
reducing points of system failure.

This chapter covers the benefits and steps needed to implement high availability clusters. Clustering sets the 
baseline for automating system failover and implementing continuous uptime. This relieves teams of performing
manual recovery techniques and creates a binary storage for the entire suite of features and components available
in your repository managers.

[[high-availability-clustering]]
=== Clustering {pro}

The minimum supported configuration for a high availability cluster is three nodes. One node represents an
individual server (the repository manager), where at least two additional servers must be joined with the
primary server to form the cluster.

When the cluster is established, nodes communicate with each other to create continuous uptime and the assurance
that if one is taken down, the other nodes can be used.

In the scenarios below joining three nodes is the minimum requirement for node redundancy. When you form a cluster,
the three nodes act as one transparent instance.

Consider the two methods to form a cluster:

* If you're setting up a new cluster, skip to <<high-availability-new,Using a Fresh Installation of {pro}>>.
* If you're converting an existing server into a cluster, skip to <<high-availability-existing,Converting an
Existing Instance of {pro} into a Cluster>>.

[[high-availability-prepare]]
=== Preparing a Cluster for High Availability

Preparing a high availability cluster requires you to:

* Configure three servers to establish the connection between the nodes by enabling high availability in each
repository manager.
* Join the nodes together by a common blob store path to maintain a shared storage for blobs, components and assets.
* Allow the first enabled node to complete the start-up sequence, then start the second and third nodes to form the
cluster.

When the cluster is formed you can <<high-availability-verify,verify node synchronization>> from 'Nodes' screen
located in 'System' from the 'Administration' menu.

[[high-availability-install]]
=== Installing {pro} for High Availability

Whether you use a fresh installation or upgrade a non-HA-supported version of {pro} to form a cluster, you
need to run the additional nodes on different hosts. Follow the appropriate scenario in this section. 

[[high-availability-new]]
==== Using a Fresh Installation of {pro}

To prepare a fresh installation of {pro} for high availability

1. Download {pro}, supported with high availability.
2. Install two additional {pro} instances on different hosts to establish three nodes.
3. Configure all three nodes to use the same <<admin-repository-blobstores,blob store>> path.

[[high-availability-existing]]
==== Converting an Existing Instance of {pro} into a Cluster

If you have an earlier version of {pro} 3, you can convert it into a cluster. Follow the steps below to
configure this non-HA-supported version so it's synchronized with the second and third nodes.

1. Stop running the existing repository manager.
2. Follow the steps in the https://support.sonatype.com/hc/en-us/articles/231723267[support article] to
upgrade the repository manager to a version that supports high availability.
3. Download and unpack a new repository manager to establish a second node.
4. Create a third instance by copying the second repository manager to establish a third node.
5. Configure all three nodes to share the same the blob stores.
6. <<high-availability-nodes,Enable>> high availability on the three nodes.
7. Start the first node and wait for its start-up sequence to complete, then start the second and third
nodes to form the cluster.

In both scenarios the data from the first node is synchronized with the second and third nodes, so that if
one node fails the other nodes in the cluster will maintain that data.

[[high-availability-blob-store]]
=== Configuring a Node to Share a Blob Store

Regardless of the method you choose to enable high availability, be mindful that configuring a single node to
share an entire `sonatype-work` directory will undermine HA configuration, and cause it not to function. All
elements of the system except the blob store needed for HA configuration should be distinct and not shared
with `sonatype-work`.

Instead, you only need to join the <<admin-repository-blobstores,blob stores>> together, where the nodes
share access to components. To configure the location where blob store data can be shared:

1. Create directory outside of the application directory, away from the `sonatype-work` directory.
2. Start your primary repository manager.
3. Choose a 'Name' and add a 'Path' from the 'Blob stores' screen, referencing the new directory you created.
4. Click 'Create blob store' 

[[high-availability-nodes]]
=== Enabling High Availability

CAUTION: In the event you have empty nodes and are adding existing configured nodes to it, the existing 
unconfigured nodes would erase the existing configuration of the nodes added. When creating a cluster, it is 
important you start the configured nodes before the empty nodes to avoid unwanted configuration loss.

When you enable high availability, the nodes discover one another via link:https://hazelcast.com/[Hazelcast].
Hazelcast, by default, employs multicast to discover cluster members, but it supports node discovery in other
ways. If the default configuration isn't suitable for your network infrastructure, you will need to customize
`$install-dir/etc/fabric/hazelcast.xml`. See <<high-availability-aws>> for a concrete example.

Follow these steps to enable high availability:

1. In the first repository manager, open the `$data-dir/etc/nexus.properties` file.
2. Remove the `#` before +nexus.clustered = true+ to enable the node at start-up. 
3. Go to the second and third repository managers and repeat steps 1 and 2, to enable them for high availability.

[[high-availability-startup]]
==== Startup and Confirming Node Connectivity

After enabling high availability for all three repository managers, check the console to confirm that multicast
discovers all three corresponding nodes.

When you start the nodes, you will see a message in the `nexus.log` confirming the connection of the cluster
members, like the one below:

----
2016-06-28 17:34:26,577-0400 INFO  [hz.nexus.generic-operation.thread-1] *SYSTEM com.hazelcast.cluster.ClusterService - [192.168.99.1]:5702 [nexus] [3.5.3]
 
Members [3] {
    Member [192.168.99.1]:5701
    Member [192.168.99.1]:5702
    Member [192.168.99.1]:5703 this
}
----

[[high-availability-verify]]
=== Verifying Synchronization

At runtime, the repository manager user interface allows you to view the status of the nodes, regardless of
which you connect to, as they are synchronized.

See <<nodes>> for details on viewing active nodes in a cluster.

[[high-availability-environment]]
=== Configuring a Cluster after Setup

Once you have your high availability environment set up, be aware that almost all configuration done via the 
user interface is shared between all nodes in the cluster. There is no master node; they are all treated
equally. For example, if you create a new repository all nodes in the cluster will be able to see it
and utilize it. Or if you want to change your 'Email Server' port you just need to do it once via the user
interface on any of the servers and the change will share.

NOTE: Same as a single server be aware, if multiple people are configuring something at the same time in your 
cluster, it may appear the changes are not sharing. If you refresh your screen, you will see the changes when 
they come across.

There are some things, however, that are not done or shared within the UI and need to be done on each individual 
server. These include:

- Any configuration files you modify (such as specifying a port via `nexus.properties` or setting up SSL)
- 'Refresh Interval' of the GUI 'Log Viewer' setting
- Most log messages are not shared across the server, however logging levels are shared
- 'Metrics' displayed are for the individual server
- A 'Support ZIP' is for the individual server. If you have issues forming a cluster, consult your support
technician and provide support zips for all nodes.
- 'Analytics' events are per server
- 'Audit' events are per server
////
last 2 should be changing with NEXUS-10489
////

TIP: Scheduled tasks will run against one node unless the 'Multi node' configuration option is selected or the 
task affects something that is in itself shared (like compaction of blob stores).

Regardless, {nxrm} configuration should not be done through the cluster's load balancer. Configuration should
occur on the individual node level.

When adding new nodes to the existing cluster be aware that they will get the shared configuration of the cluster 
regardless of how they are preconfigured.

[[high-availability-aws]]
=== Configuring High Availability for Amazon Web Services

{nxrm} can be deployed on cloud-computing services, such as Amazon Web Services (AWS). Depending on your network
security, additional configuration may be required. For example, if you use a network layer firewall application
it may block multicast communication. If such a failure occurs you will need to modify the Hazelcast configuration
file.

To configure Hazelcast for automatic node discovery find the `<join>` tag in `$install-dir/etc/fabric/hazelcast.xml`.
Then, edit the file for each node:

1. Change the value in `<multicast enabled="true">` to `"false"`.
2. Change the value in `<aws enabled="false">` to `"true"`.
3. Save the file.
4. Reboot each node in the cluster.

The `$install-dir/etc/fabric/hazelcast.xml` file with the modified properties will look similar to this:
----
<join>
    <multicast enabled="false">
       <multicast-group>224.2.2.3</multicast-group>
       <multicast-port>54327</multicast-port>
    </multicast>
    <tcp-ip enabled="false">
        <interface>127.0.0.1</interface>
    </tcp-ip>
    <aws enabled="true">
        <access-key>my-access-key</access-key>
        <secret-key>my-secret-key</secret-key>
        <!--optional, default is us-east-1 -->
        <region>us-west-1</region>
        <!--optional, default is ec2.amazonaws.com. If set, region shouldn't be set as it will override this property -->
        <host-header>ec2.amazonaws.com</host-header>
        <!-- optional, only instances belonging to this group will be discovered, default will try all running instances -->
        <security-group-name>security-group-name</security-group-name>
        <tag-key>type</tag-key>
        <tag-value>nexus-nodes</tag-value>
    </aws>
</join>
----
